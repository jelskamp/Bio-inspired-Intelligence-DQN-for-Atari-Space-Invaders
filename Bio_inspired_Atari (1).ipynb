{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzdekkMPNWMX"
      },
      "source": [
        "## Imports and Google Drive Connect ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74fER-Vg82uy"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install torch torchvision stable_baselines3\n",
        "!pip install gymnasium[accept-ROM-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LilBm6k0NLrS"
      },
      "outputs": [],
      "source": [
        "# Mount to Google Drive for storing runs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4MuYYnd8zk7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO, DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.logger import HParam, configure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZlzz1gN8zk8"
      },
      "source": [
        "## CNN Policiy ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e_uWvlZ8zk9"
      },
      "outputs": [],
      "source": [
        "# Create all CNN options\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        # We assume CxHxW images (channels first)\n",
        "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),)\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CustomResCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomResCNN, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.cnn3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.flatten(\n",
        "                self.cnn3(self.cnn2(self.cnn1(th.as_tensor(observation_space.sample()[None]).float())))).shape[1]\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        x = self.cnn1(observations)\n",
        "        x = self.cnn2(x)\n",
        "        x = x + self.cnn3(x)  # Residual connection\n",
        "        x = self.flatten(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CustomDeepCNN(BaseFeaturesExtractor):\n",
        "\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomDeepCNN, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CustomMaxPoolCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomMaxPoolCNN, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(\n",
        "                th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cErll6FG8zk9"
      },
      "source": [
        "## Load Environment ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcVQ02jp8zk9"
      },
      "outputs": [],
      "source": [
        "environment_name = 'SpaceInvadersNoFrameskip-v4'\n",
        "env = gym.make(environment_name, render_mode=\"rgb_array\")\n",
        "env.metadata['render_fps'] = 10000\n",
        "env = Monitor(env)\n",
        "env = gym.wrappers.AtariPreprocessing(env, noop_max = 30, frame_skip = 4, screen_size = 84, terminal_on_life_loss = False, grayscale_obs = True, grayscale_newaxis = False, scale_obs = False)\n",
        "env = gym.wrappers.FrameStack(env, 4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmfLTpW38zk-"
      },
      "outputs": [],
      "source": [
        "# ONLY USED FOR LEARNING/TESTING, NOT USED IN REPORT:\n",
        "# episodes = 2\n",
        "# mean_score = 0\n",
        "# for episode in range(1, episodes+1):\n",
        "#     obs = env.reset()                                                   #returns initial observation\n",
        "#     done = False\n",
        "#     score = 0\n",
        "\n",
        "#     while not done:\n",
        "#         env.render()\n",
        "#         action = env.action_space.sample()                              #take sample of action space\n",
        "#         obs, reward, terminated, truncated, info = env.step(action)     #take step and obtain obs,reward,etc...\n",
        "#         score += reward                                                 #add up score\n",
        "\n",
        "#         done = terminated or truncated\n",
        "#     print('Episode:{} Score:{}'.format(episode, score))\n",
        "#     mean_score += score\n",
        "\n",
        "# print(mean_score/episodes)\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S0Ow0qt8zk-"
      },
      "source": [
        "## Create Model ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrjOIMH38zk-"
      },
      "outputs": [],
      "source": [
        "# Create log path\n",
        "log_path = os.path.join('drive', 'MyDrive', 'Bio_Inspired_Intelligence', 'Training', 'Logs_v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWhR8QZH8zk-"
      },
      "outputs": [],
      "source": [
        "# Current settings: default\n",
        "\n",
        "dqn_hyperparams = {\n",
        "    'learning_rate': 1e-3,                  # default = 1e-3\n",
        "    'buffer_size': 10000,                   # default = 10000\n",
        "    'learning_starts': 1000,                # default = 1000\n",
        "    'batch_size': 32,                       # default=32, mini batch size\n",
        "    'tau': 1.0,                             # for soft update of target parameters, default = 1.0\n",
        "    'gamma': 0.99,                          # discount factor, default = 0.99\n",
        "    'train_freq': 4,\n",
        "    'gradient_steps': 1,\n",
        "    'target_update_interval': 10000,        # default = 10000\n",
        "    'exploration_fraction': 0.1,            # default = 0.1\n",
        "    'exploration_final_eps': 0.02,          # default = 0.02\n",
        "    'exploration_initial_eps': 1.0,         # default = 1.0\n",
        "    'verbose': 1,\n",
        "    'tensorboard_log': log_path,\n",
        "\n",
        "    'policy_kwargs': {\n",
        "        'features_extractor_class': CustomDeepCNN,\n",
        "        'features_extractor_kwargs': {'features_dim':128},\n",
        "        'activation_fn': th.nn.ReLU\n",
        "    },\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# Create the DQN model with the defined hyperparameters\n",
        "model = DQN('CnnPolicy', env, **dqn_hyperparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQj_9UwW8zk_"
      },
      "source": [
        "## Train, Save & Reload Model ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To32o_zp8zk_"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/', name_prefix='rl_model')\n",
        "eval_callback = EvalCallback(env, best_model_save_path='./logs/', log_path='./logs/', eval_freq=500, deterministic=True, render=False)\n",
        "\n",
        "# tb_log_name = 'model_save_default_CustomDeepCNN'\n",
        "tb_log_name = 'model_save_FINALv3_CustomDeepCNN'\n",
        "\n",
        "model.learn(total_timesteps=300000, tb_log_name=tb_log_name, callback=[checkpoint_callback, eval_callback])            # run first time\n",
        "\n",
        "# saved_model = 'DQN_model_default_CustomDeepCNN'\n",
        "saved_model = 'DQN_model_FINALv3_CustomDeepCNN'\n",
        "\n",
        "\n",
        "# Make path\n",
        "DQN_path = os.path.join('drive', 'MyDrive', 'Bio_Inspired_Intelligence', 'Training', 'Saved Models')\n",
        "model.save(DQN_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9ZHaAeH8zk_"
      },
      "outputs": [],
      "source": [
        "tb_log_name = 'model_save_FINALv3_CustomDeepCNN'\n",
        "saved_model = 'DQN_model_FINALv3_CustomDeepCNN'\n",
        "del model\n",
        "\n",
        "DQN_path = os.path.join('drive', 'MyDrive', 'Bio_Inspired_Intelligence', 'Training', 'Saved Models')\n",
        "model = DQN.load(DQN_path, env=env)\n",
        "\n",
        "# use this every time you 're-train':\n",
        "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/', name_prefix='rl_model')\n",
        "eval_callback = EvalCallback(env, best_model_save_path='./logs/', log_path='./logs/', eval_freq=500, deterministic=True, render=False)\n",
        "\n",
        "# Retrain\n",
        "model.set_env(env)\n",
        "model.learn(total_timesteps=50000, tb_log_name=tb_log_name, callback=[checkpoint_callback, eval_callback], reset_num_timesteps=False) # run next steps to build upon initial\n",
        "\n",
        "# Save final model\n",
        "model.save(DQN_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xefmmg9j8zk_"
      },
      "source": [
        "## Evaluation ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTWC8I9P8zk_"
      },
      "outputs": [],
      "source": [
        "# ONLY USED FOR LEARNING/TESTING, NOT USED IN REPORT:\n",
        "# episodes = 100\n",
        "# acc_score = 0\n",
        "# for episode in range(1, episodes+1):\n",
        "#     obs = env.reset()\n",
        "#     done = False\n",
        "#     score = 0\n",
        "\n",
        "\n",
        "#     while not done:\n",
        "#         env.render()\n",
        "#         action, _ = model.predict(obs)     # NOW USING MODEL HERE\n",
        "#         # obs, reward, terminated, truncated, info = env.step(action)\n",
        "#         obs, reward, done, info = env.step(action)\n",
        "#         score += reward\n",
        "\n",
        "#         # done = terminated or truncated\n",
        "#     print('Episode:{} Score:{}'.format(episode, score))\n",
        "#     acc_score += score\n",
        "\n",
        "# print(acc_score/episodes)\n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep3sETulEVkj"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir drive/MyDrive/Bio_Inspired_Intelligence/Training/Logs_v2/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2wCEYFdIsid"
      },
      "source": [
        "## Uncertainty Analysis ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMCcBhn0IsUO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X = 3\n",
        "results_1 = []\n",
        "results_2 = []\n",
        "results_3 = []\n",
        "\n",
        "# Run the training and evaluation process X times\n",
        "for i in range(X):\n",
        "    tb_log_name = f'u/model_save_FINALv3_Uncertainty_{i}'\n",
        "    saved_model = f'u/DQN_model_FINALv3_Uncertainty_{i}'\n",
        "\n",
        "    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=f'./logs/{saved_model}', name_prefix='rl_model')\n",
        "    eval_callback = EvalCallback(env, best_model_save_path=f'./logs/{saved_model}', log_path=f'./logs/{tb_log_name}', eval_freq=500, deterministic=True, render=False)\n",
        "\n",
        "    # Create and train the model\n",
        "    model = DQN('CnnPolicy', env, **dqn_hyperparams)\n",
        "    model.learn(total_timesteps=50000, tb_log_name=tb_log_name, callback=[checkpoint_callback, eval_callback])\n",
        "\n",
        "    # Evaluate the model\n",
        "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "\n",
        "    if i == 0:\n",
        "        results_1.append(mean_reward)\n",
        "    elif i == 1:\n",
        "        results_2.append(mean_reward)\n",
        "    elif i == 2:\n",
        "        results_3.append(mean_reward)\n",
        "\n",
        "# Print the results from each run\n",
        "print(\"Results from model run 1:\", results_1)\n",
        "print(\"Results from model run 2:\", results_2)\n",
        "print(\"Results from model run 3:\", results_3)\n",
        "\n",
        "# Calculate the average results list from the three runs\n",
        "average_results = np.mean([results_1, results_2, results_3], axis=0)\n",
        "\n",
        "# Print the average results\n",
        "print(\"\\nAverage Results List:\", average_results)\n",
        "\n",
        "# Calculate the mean squared error between the average results and each run's results\n",
        "mse_1 = mean_squared_error(average_results, results_1)\n",
        "mse_2 = mean_squared_error(average_results, results_2)\n",
        "mse_3 = mean_squared_error(average_results, results_3)\n",
        "\n",
        "# Relative MSE to mean squared\n",
        "mse_1_rel = mse_1 / (average_results ** 2)\n",
        "mse_2_rel = mse_2 / (average_results ** 2)\n",
        "mse_3_rel = mse_3 / (average_results ** 2)\n",
        "\n",
        "# Print the mean squared errors\n",
        "print(f\"\\nRelative Mean Squared Error between Average and Run 1: {mse_1_rel}\")\n",
        "print(f\"Relative Mean Squared Error between Average and Run 2: {mse_2_rel}\")\n",
        "print(f\"Relative Mean Squared Error between Average and Run 3: {mse_3_rel}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}